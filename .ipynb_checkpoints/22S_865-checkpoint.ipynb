{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 2022S 865, Individual Assignment 1\n",
    "\n",
    "Version 2: Updated January 1, 2022.\n",
    "\n",
    "- Gopala Goyal\n",
    "- 20254605\n",
    "- 2\n",
    "- January 8, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - ELI5\n",
    "\n",
    "_“If you can't explain it simply, you don't understand it well enough.” – Albert Einstein_\n",
    "\n",
    "Explaining technical concepts to a non-technical audience is an underappreciated skill; one which the GMMA/MMA program aims to give its students; and one that will truly set you apart in the job market. The only way to gain a skill is by practice, so here we go.\n",
    "\n",
    "Answer each question below as though you were talking to a 5 year old (equivalently: a grandma, or a completely non-technical manager, or an Ivey grad). Use your own words. Use analogies where possible. Examples are better than theory. Keep it short, but be complete. Use simple, plain English. Do not use business buzzwords like _actualize, empower, fungible, leverage, or synergize_. Do not use technical buzzwords that most people don’t know like _model, agile, bandwidth, IoT, blockchain, AR, VR, actionable insights_. Inform the audience without going into too much technical detail. Your goal is to truly help them understand, not to give what you feel is a “technically precise” answer and move on (but they still don’t understand!). Don’t be that guy!\n",
    "\n",
    "Please keep each answer to 1000 characters or less.\n",
    "\n",
    "Finally, feel free to use [Markdown syntax](https://www.markdownguide.org/basic-syntax/) to format your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: What is “Big Data” and how is it different than “regular data”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: What is Hadoop? Hint: What problems in previous data storage and processing was Hadoop designed to solve? How did Hadoop accomplish that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: How does Big Data and the cloud help Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What is NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Name three ways topic modeling could help a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: What is Apache Spark, exactly, and what are its pros and cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package pe08 to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /Users/apple/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  600 non-null    object\n",
      " 1   Polarity  600 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0  A good commentary of today's love and undoubte...         1\n",
      "1  For people who are first timers in film making...         1\n",
      "2  It was very popular when I was in the cinema, ...         1\n",
      "3  It's a feel-good film and that's how I felt wh...         1\n",
      "4  It has northern humour and positive about the ...         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# TODO: import other libraries as necessary\n",
    "\n",
    "df_train = pd.read_csv(\"sentiment_train.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"sentiment_test.csv\")\n",
    "\n",
    "print(df_test.info())\n",
    "print(df_test.head())\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.Polarity.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "# Tokenization\n",
    "# Removing Stop Words\n",
    "# Removing punctuation and character\n",
    "# Spelling correction\n",
    "# Lemmatization or Stemming - Standardizing the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing/wrong values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['review_length']  = df_train.Sentence.str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>DELICIOUS!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>Incredible!.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Disappointed!.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>disappointing.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>WORTHWHILE.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>Excellent!.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>disappointed.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>Brilliant!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>10-Oct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>Horrible!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>Awful.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence  Polarity  review_length\n",
       "26            #NAME?         0              1\n",
       "71            #NAME?         1              1\n",
       "165      DELICIOUS!!         1              1\n",
       "219           #NAME?         1              1\n",
       "904           #NAME?         0              1\n",
       "1185    Incredible!.         1              1\n",
       "1463  Disappointed!.         0              1\n",
       "1751  disappointing.         0              1\n",
       "1777     WORTHWHILE.         1              1\n",
       "1877     Excellent!.         1              1\n",
       "1993   disappointed.         0              1\n",
       "2124    Brilliant!           1              1\n",
       "2125          10-Oct         1              1\n",
       "2155     Horrible!           0              1\n",
       "2162        Awful.           0              1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.review_length==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to remove the rows with data as '#NAME?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(df_train[df_train.Sentence == \"#NAME?\"].index).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Almost all of the songs in Cover Girl are old-...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>The most annoying thing about 'Cover Girl' is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Unfortunately, 'Cover Girl' is an example of h...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>Non-linear narration thus many flashbacks and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>The good cinematography also makes her and Mon...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2396 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Polarity  \\\n",
       "0                              Wow... Loved this place.         1   \n",
       "1                                    Crust is not good.         0   \n",
       "2             Not tasty and the texture was just nasty.         0   \n",
       "3     Stopped by during the late May bank holiday of...         1   \n",
       "4     The selection on the menu was great and so wer...         1   \n",
       "...                                                 ...       ...   \n",
       "2391  Almost all of the songs in Cover Girl are old-...         0   \n",
       "2392  The most annoying thing about 'Cover Girl' is ...         0   \n",
       "2393  Unfortunately, 'Cover Girl' is an example of h...         0   \n",
       "2394  Non-linear narration thus many flashbacks and ...         1   \n",
       "2395  The good cinematography also makes her and Mon...         1   \n",
       "\n",
       "      review_length  \n",
       "0                 4  \n",
       "1                 4  \n",
       "2                 8  \n",
       "3                15  \n",
       "4                12  \n",
       "...             ...  \n",
       "2391             14  \n",
       "2392             20  \n",
       "2393             16  \n",
       "2394             12  \n",
       "2395             12  \n",
       "\n",
       "[2396 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to clean data\n",
    "#Remove punctuations\n",
    "#Remove numbers\n",
    "#Lowercase the string\n",
    "#Split the strings on white spaces\n",
    "#Remove Stopwords\n",
    "#Lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def text_cleaner(text):\n",
    "    r = text.translate(str.maketrans(\"\",\"\",string.punctuation)) \n",
    "    r = re.sub(r'\\d+', '',r) \n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "    r = ' '.join(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_sentence'] = df_train.Sentence.apply(lambda text:text_cleaner(text))\n",
    "df_train['review_length']  = df_train.clean_sentence.str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns  = [\"Polarity\"])\n",
    "y = df_train.Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X_cv = cv.fit_transform(X.clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.DataFrame(data=X_cv.toarray(), columns = cv.get_feature_names())],axis = 1).drop(columns = ['Sentence','clean_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.33417507, 0.4701615 ])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.ability.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Feel free to use code as well to answer this question. Or not. Up to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
